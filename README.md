# Real-Time Data Processing and Visualization Pipeline

*Generated by DALL-E, not an actual architecture diagram*
![dall-e diagram](/docs/images/DALL_E-Architecture_diagram.png)

## Introduction
This project is the capstone of the 14-week [Data Engineer Camp](https://dataengineercamp.com/), focusing on building a real-time data processing and visualization pipeline using [Wikimedia eventstreams](https://wikitech.wikimedia.org/wiki/Event_Platform/EventStreams). The project integrates various modern data engineering tools to create a comprehensive pipeline from data ingestion to insightful visualizations.

## Objectives and Goals
The primary goal was to build a real-time data processing system that can handle streaming data, perform NER analysis using neural networks, and visualize the results effectively. The project aims to demonstrate skills in ETL/ELT pipelines, cloud hosting, big data processing with Databricks/Pyspark, and data visualization.

# Business Goal

## Overview
This project's business goal centers around monitoring and analyzing newly added content to Wikimedia pages. By leveraging real-time data processing and advanced neural network-based Named Entity Recognition (NER), the project provides insightful analytics on the nature and distribution of content being added across various Wikimedia domains.

## Key Objectives
Monitoring Content in Real-Time: Track newly created or uploaded content to Wikimedia in real-time, providing an up-to-date overview of the latest additions and updates.
Domain Analysis: Identify the most active Wikimedia domains, helping to understand which areas are witnessing the most frequent content updates or additions.
Content Categorization: Utilize NER to categorize content based on predefined entities. This categorization aids in understanding the types of content that are popular or trending in different domains.

## NER Categories and Their Significance
The project focuses on the following NER categories:

 - PERSON: Identification of people (including fictional characters) mentioned in new content, indicating popular figures or characters being discussed.
 - DATE: Tracking the occurrence of specific dates, which can highlight content related to historical events, anniversaries, or significant occurrences.
 - WORK_OF_ART: Titles of books, songs, and other artistic works, offering insights into popular cultural references being added.
 - ORG: Names of companies, agencies, and institutions, useful for tracking mentions of organizations, which can be indicative of current events or organizational impacts.

![Result visualization](/docs/images/visualization.png)

# Technology Overview

## Introduction

The capstone project harnesses a variety of advanced data engineering and processing technologies to build a real-time data pipeline for analyzing and visualizing Wikimedia event streams. The technology stack was carefully chosen to handle the demands of real-time data processing, from ingestion to analysis and visualization.

# Key Technologies and Their Roles

### AWS ECS (Elastic Container Service) and ECR (Elastic Container Registry)
 - **Role**: Provides a robust and scalable cloud platform for hosting and running Dockerized applications. ECS is a highly scalable and fast container management service that makes it easy to run, stop, and manage Docker containers on a cluster.
 - **Implementation**: Used for hosting the python scripts that ingest Wikimedia event streams into Kafka. The scripts are containerized using Docker and deployed on ECS.

### Kafka (Confluent Cloud)
 - **Role**: Acts as the central messaging system for real-time data streams. Kafka provides high throughput and scalable infrastructure to handle the streaming data from Wikimedia events.
 - **Implementation**: Utilized for ingesting Wikimedia event streams and for distributing processed data for further analysis and storage.

### Databricks and PySpark
 - **Role**: Provides a powerful platform for big data processing and analytics. PySpark, the Python API for Apache Spark, is used for its ability to handle large-scale data processing.
 - **Implementation**: Employed for processing the data streamed from Kafka. PySpark is particularly crucial for implementing the NER model, which is a key part of the project's data enrichment phase.

### ClickHouse
 - **Role**: Serves as the primary database for storing processed data. ClickHouse is known for its speed and efficiency in handling large volumes of data, making it an ideal choice for real-time analytics.
 - **Implementation**: Stores both raw and processed data from Kafka. This includes direct streams of Wikimedia events and enriched data after NER processing.

### Preset (Superset)
 - **Role**: Used for data visualization and business intelligence. Preset, which is built on Apache Superset, allows for the creation of interactive dashboards and reports.
 - **Implementation**: Visualizes the real-time data stored in ClickHouse, providing insights into Wikimedia event streams and NER analysis results.

### Neural Network-Based NER
 - **Role**: Essential for the project's data enrichment process. It categorizes and extracts specific entities from the Wikimedia content.
 - **Implementation**: Integrated within the Databricks/PySpark setup, this NER model processes the text data from Wikimedia events to identify and classify named entities.

## Architecture

The architecture of the project is designed to facilitate seamless data flow and efficient processing, encompassing a range of cloud-based services, data processing tools, and visualization technologies. The system follows these steps:

![Architecture diagram](/docs/images/architecture.png)

1. **Data Ingestion via AWS ECS/ECR and Python Script**:
   - A Dockerized Python script, hosted on AWS ECS and stored in AWS ECR, reads real-time data from the Wikimedia EventStream.
   - Utilizing `sseclient`, the script establishes a connection to Wikimedia's Server-Sent Events (SSE) streams to continuously receive new events.
   - The script processes the incoming data and publishes it to a Kafka topic hosted in Confluent Cloud.

2. **Initial Data Preprocessing in Kafka**:
   - The raw data from Wikimedia streams, now in Kafka, undergoes initial preprocessing using ksqlDB.
   - This preprocessing step involves flattening JSON structures and extracting relevant fields, preparing the data for further analysis.

3. **Data Processing and Enrichment with Databricks/PySpark**:
   - The preprocessed data is streamed from Kafka into Databricks.
   - PySpark, within the Databricks environment, applies a pretrained NER model to enrich the data by identifying and classifying named entities.

4. **Storage in ClickHouse**:
   - The enriched data, along with the raw stream data, is stored in ClickHouse.
   - ClickHouse serves as the data warehouse, optimized for handling large volumes of data and facilitating fast query performance.

5. **Real-time Data Visualization with Preset**:
   - The data stored in ClickHouse is visualized using Preset (Superset).
   - The visualizations provide real-time insights into the Wikimedia content, including volume metrics, domain activity, and NER analysis results.

# How to Run the Project
Running this project involves a series of steps that integrate various cloud services and technologies. Below is a high-level overview of how to get the project running:

## Prerequisites
 - Access to AWS for ECR and ECS services.
 - An account on Confluent Cloud for Kafka services.
 - Access to Databricks Community Edition.
 - Access to ClickHouse and Preset instances.
 - The project's source code is available on GitHub.

## Steps
1. GitHub Actions for Continuous Integration
   - GitHub Actions is set up to automate the building of the Docker container, which reads Wikimedia EventStream data.
   - Upon a push to the repository, GitHub Actions builds the Docker image and pushes it to AWS ECR.
2. Terraform for AWS Infrastructure
   - Terraform scripts are used to set up the AWS environment. This includes provisioning ECR for storing the Docker image and ECS for running the containerized task.
   - Run the Terraform scripts to create the necessary AWS resources.
3. Initialize Databricks Environment
   - Since the project uses Databricks Community Edition, which auto-terminates after one hour of inactivity, the computing resources need to be initialized manually when needed.
   - Log in to Databricks and start the required computing resources.
4. Install Required Libraries in Databricks
   - Install the following libraries in your Databricks environment:
     * Spark NLP (spark-nlp Python library)
     * Spark NLP for Apache Spark (com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4 Maven library)
     * ClickHouse JDBC (com.clickhouse:clickhouse-jdbc:0.5.0 Maven library)
     * Hadoop AWS (org.apache.hadoop:hadoop-aws:3.2.0 Maven library)
5. Run the PySpark Notebook
   - Once the computing resources are up and the libraries are installed, manually run the PySpark notebook to process the Wikimedia events.
